---
title: Testing y buenas prÃ¡cticas
description: Estrategias de testing y buenas prÃ¡cticas en PySpark.
---

### ğŸ§  Â¿Por quÃ© es importante?

En proyectos con PySpark, es esencial garantizar que las transformaciones de datos sean correctas, reproducibles y escalables. Hacer **testing** y seguir **buenas prÃ¡cticas** ayuda a:

* Detectar errores temprano.
* Mantener cÃ³digo limpio y mantenible.
* Facilitar colaboraciÃ³n entre equipos.
* Prevenir errores en producciÃ³n.

### âœ… Testing en PySpark

#### ğŸ§ª 1. Unit testing (Pruebas unitarias)

Se enfoca en probar funciones y transformaciones individuales sobre pequeÃ±os DataFrames controlados.

**Ejemplo con `pytest`:**

```python
from pyspark.sql import SparkSession
import pytest

@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder.master("local[1]").appName("test").getOrCreate()

def test_filter_adultos(spark):
    data = [("Ana", 17), ("Luis", 22)]
    df = spark.createDataFrame(data, ["nombre", "edad"])
    
    resultado = df.filter(df.edad >= 18)
    
    assert resultado.count() == 1
    assert resultado.collect()[0]["nombre"] == "Luis"
```

#### ğŸ§¬ 2. DataFrame equality (comparar resultados esperados)

Puedes verificar que el resultado esperado y el obtenido tengan los mismos datos, sin importar el orden:

```python
from pyspark.sql import Row

esperado = spark.createDataFrame([Row(nombre="Luis", edad=22)])
resultado = df.filter(df.edad >= 18)

assert sorted(esperado.collect()) == sorted(resultado.collect())
```

### ğŸ§­ Buenas prÃ¡cticas en PySpark

#### ğŸ“ 1. OrganizaciÃ³n del cÃ³digo

* Separa el cÃ³digo en **mÃ³dulos reutilizables**: lectura, transformaciÃ³n, validaciÃ³n, escritura.
* Usa funciones puras y evita efectos secundarios en transformaciones.
* Usa `main()` y `if __name__ == "__main__"` para ejecutar scripts.

#### ğŸ“š 2. DocumentaciÃ³n

* Documenta las funciones, especialmente las que transforman datos.
* Incluye ejemplos de entrada/salida.

```python
def limpiar_nombres(df):
    """
    Limpia espacios y pone en minÃºscula los nombres de los clientes.
    
    ParÃ¡metros:
        df (DataFrame): DataFrame con columna 'nombre'.
    
    Retorna:
        DataFrame con columna 'nombre' limpia.
    """
    return df.withColumn("nombre", lower(trim(col("nombre"))))
```

#### ğŸ” 3. Validaciones y chequeos

* Verifica esquema de entrada con `df.printSchema()`.
* Comprueba valores nulos antes de procesar.
* Usa `assert` o logs para validar supuestos.

```python
assert "monto" in df.columns, "Falta la columna monto"
```

#### ğŸ§° 4. Uso de funciones integradas de Spark

Evita usar `collect()` innecesariamente (ya que trae los datos al driver). Usa funciones como:

* `withColumn()`
* `selectExpr()`
* `filter()`
* `groupBy().agg()`

Estas son mÃ¡s eficientes y escalan mejor que `map()` con UDFs.

#### ğŸ“¦ 5. Uso de formatos eficientes

* Prefiere `parquet`, `delta`, o `orc` para almacenamiento.
* Evita `csv` para grandes volÃºmenes.

#### ğŸ“Š 6. Uso de particiones y cache adecuadamente

* Aplica `.repartition()` o `.coalesce()` segÃºn el caso.
* Usa `.cache()` si vas a reutilizar un DataFrame en varias operaciones.

#### ğŸ§¼ 7. Limpieza del entorno de pruebas

Siempre detÃ©n el `SparkSession` despuÃ©s de pruebas para evitar conflictos:

```python
spark.stop()
```

### ğŸ§ª Herramientas recomendadas

* **pytest**: framework de testing.
* **hypothesis**: testing con generaciÃ³n de datos aleatorios.
* **assertpy** o `assert` bÃ¡sico.
* **CI/CD**: Integrar pruebas automÃ¡ticas con GitHub Actions o GitLab CI.

### ğŸ“˜ Ejemplo de estructura ideal del proyecto

```
mi_proyecto_pyspark/
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ transformaciones/
â”‚   â””â”€â”€ limpieza.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_limpieza.py
â”œâ”€â”€ datos/
â”‚   â””â”€â”€ ejemplo.csv
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```