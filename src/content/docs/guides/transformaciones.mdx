---
title: Transformaciones bÃ¡sicas
description: Operaciones fundamentales para manipular datos en PySpark.
---

Las **transformaciones** en PySpark te permiten modificar, limpiar o preparar tus datos. Son operaciones **perezosas** (lazy), lo que significa que no se ejecutan hasta que aplicas una acciÃ³n como `show()`, `collect()` o `write()`.

### ğŸ”¹ `select()` â€“ Seleccionar columnas

```python
df.select("nombre", "edad")
```

TambiÃ©n puedes renombrar columnas:

```python
from pyspark.sql.functions import col

df.select(col("nombre").alias("nombre_completo"))
```

### ğŸ”¹ `withColumn()` â€“ Crear o modificar columnas

```python
df.withColumn("edad_doble", col("edad") * 2)
```

âœ… Reemplaza una columna si ya existe, o la crea si no.

### ğŸ”¹ `filter()` y `where()` â€“ Filtrar registros

```python
df.filter(col("edad") > 25)
df.where((col("edad") > 25) & (col("ciudad") == "Lima"))
```

Ambos mÃ©todos son equivalentes.

### ğŸ”¹ `drop()` â€“ Eliminar columnas

```python
df.drop("columna_innecesaria")
```

### ğŸ”¹ `distinct()` â€“ Eliminar duplicados

```python
df.select("ciudad").distinct()
```

### ğŸ”¹ `dropDuplicates()` â€“ Eliminar duplicados por columnas especÃ­ficas

```python
df.dropDuplicates(["nombre", "edad"])
```

### ğŸ”¹ `orderBy()` y `sort()` â€“ Ordenar filas

```python
df.orderBy(col("edad").desc())
df.sort("nombre", "edad")
```

### ğŸ”¹ `limit()` â€“ Limitar el nÃºmero de filas

```python
df.limit(10).show()
```

### ğŸ”¹ `groupBy()` + `agg()` â€“ Agrupar y agregar

```python
from pyspark.sql.functions import avg, max, count

df.groupBy("departamento").agg(
    avg("salario").alias("salario_promedio"),
    max("salario").alias("salario_maximo"),
    count("*").alias("total_empleados")
)
```

### ğŸ”¹ `join()` â€“ Unir DataFrames

```python
df1.join(df2, on="id", how="inner")  # left, right, outer tambiÃ©n disponibles
```

Puedes especificar mÃºltiples columnas con una lista:

```python
df1.join(df2, on=["id", "fecha"], how="left")
```

### ğŸ”¹ `union()` â€“ Combinar filas de dos DataFrames

```python
df1.union(df2)
```

âš ï¸ Los DataFrames deben tener el **mismo nÃºmero y orden de columnas**.

### ğŸ”¹ `withColumnRenamed()` â€“ Renombrar columnas

```python
df.withColumnRenamed("nombre_viejo", "nombre_nuevo")
```

### ğŸ”¹ `alias()` â€“ Renombrar columnas temporalmente

```python
df.select(col("salario").alias("sueldo"))
```

### ğŸ”¹ `isNull()` y `isNotNull()` â€“ Detectar valores nulos

```python
df.filter(col("email").isNull())
df.filter(col("email").isNotNull())
```

### ğŸ›  Ejemplo prÃ¡ctico combinado

```python
from pyspark.sql.functions import col, when

df_clean = df.filter(col("edad").isNotNull()) \
    .withColumn("nivel", when(col("edad") < 18, "menor")
                .otherwise("adulto")) \
    .drop("telefono") \
    .orderBy(col("edad").desc())
```

### âœ… Buenas prÃ¡cticas

* Evita crear mÃºltiples `withColumn()` encadenados si puedes hacer todo en uno.
* Usa `select()` para seleccionar solo las columnas necesarias (mejor performance).
* Evita usar `collect()` con muchos datos: mueve todo a memoria local.