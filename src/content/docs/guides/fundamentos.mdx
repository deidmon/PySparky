---
title: Fundamentos de PySpark
description: Conceptos clave y arquitectura de PySpark.
---


Esta secci칩n cubre los conceptos esenciales para entender c칩mo funciona PySpark, qu칠 tipo de estructuras maneja, y c칩mo manipular tus datos de forma inicial.

### 游댳 쯈u칠 es un DataFrame en PySpark?

Un **DataFrame** es una colecci칩n distribuida de datos estructurados, similar a una tabla en SQL o un `DataFrame` en pandas.

* Soporta operaciones como: `select`, `filter`, `join`, `groupBy`.
* Es inmutable (como RDDs): cada transformaci칩n genera uno nuevo.
* Se ejecuta de forma **lazy** (diferida): no corre hasta que llamas una acci칩n como `show()`, `count()`, `collect()`.

### 游댳 PySpark vs pandas

| Caracter칤stica | pandas | PySpark |
|---|---|---|
| Escala | Memoria local | Datos distribuidos |
| Velocidad (peque침os) | Muy r치pida | M치s lenta que pandas |
| Velocidad (grandes) | Se cae | Alta performance |
| APIs | Familiar | Similar a SQL |

### 游댳 쯈u칠 es un RDD?

Un **RDD** (Resilient Distributed Dataset) es la estructura m치s b치sica de Spark. Es una colecci칩n inmutable y distribuida de objetos de bajo nivel.

丘멆잺 Usa RDDs solo cuando necesites manipular datos a nivel muy bajo o no estructurados.

### 游댳 Crear un DataFrame

#### Desde una lista de diccionarios

```python
datos = [{"nombre": "Juan", "edad": 30}, {"nombre": "Ana", "edad": 25}]
df = spark.createDataFrame(datos)
df.show()
```

#### Desde un archivo

```python
df = spark.read.csv("clientes.csv", header=True, inferSchema=True)
```

### 游댳 Inspecci칩n de datos

```python
df.show(5)          # Muestra las primeras filas
df.printSchema()    # Muestra el esquema
df.columns          # Lista de columnas
df.dtypes           # Tipos de datos
df.describe().show() # Estad칤sticas b치sicas
```

### 游댳 Acciones vs Transformaciones

| Tipo | Ejemplo | 쯈u칠 hace? |
|---|---|---|
| Transformaci칩n | `select`, `filter` | Devuelve un nuevo DataFrame (no ejecuta nada a칰n) |
| Acci칩n | `show`, `count` | Ejecuta el plan de ejecuci칩n y devuelve resultados |

Ejemplo:

```python
df2 = df.filter(df.edad > 25)  # TRANSFORMACI칍N
df2.show()                     # ACCI칍N (aqu칤 se ejecuta)
```

### 游댳 Esquemas: Tipado expl칤cito

Puedes definir el esquema manualmente para mayor control:

```python
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

esquema = StructType([
    StructField("nombre", StringType(), True),
    StructField("edad", IntegerType(), True)
])

df = spark.read.schema(esquema).json("clientes.json")
```

### 游댳 Conversi칩n entre estructuras

#### De pandas a PySpark

```python
import pandas as pd

pdf = pd.DataFrame({"nombre": ["Juan"], "edad": [30]})
df = spark.createDataFrame(pdf)
```

#### De PySpark a pandas

```python
df_pandas = df.toPandas()
```

丘멆잺 Evita usar `toPandas()` con grandes vol칰menes de datos. Puede colapsar tu RAM.

### 游댳 Buenas pr치cticas

* Siempre inspecciona el esquema (`printSchema`) antes de hacer transformaciones.
* Si trabajas con archivos grandes, usa `inferSchema=False` y define t칰 mismo el esquema.
* Prefiere usar DataFrames sobre RDDs siempre que sea posible.