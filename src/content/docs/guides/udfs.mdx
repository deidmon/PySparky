---
title: UDFs y funciones personalizadas
description: C√≥mo crear y usar UDFs en PySpark.
---

En esta secci√≥n se abordan las **User-Defined Functions (UDFs)** y otras formas de crear funciones personalizadas para extender las capacidades de PySpark m√°s all√° de las funciones SQL integradas.

### üîç ¬øQu√© es una UDF?

Una **UDF (User Defined Function)** es una funci√≥n personalizada escrita en Python (u otro lenguaje compatible) que puede aplicarse a columnas de un DataFrame en PySpark cuando las funciones integradas no son suficientes.

‚ö†Ô∏è Las UDFs pueden afectar el rendimiento porque rompen la optimizaci√≥n del plan de ejecuci√≥n de Spark y no pueden ser compiladas a c√≥digo eficiente como las funciones integradas.

### ‚úçÔ∏è C√≥mo crear y usar una UDF

#### Paso 1: Definir la funci√≥n

Define una funci√≥n normal de Python:

```python
def mayuscula(texto):
    return texto.upper()
```

#### Paso 2: Convertirla en UDF

Utiliza `F.udf()` y especifica el tipo de dato de salida:

```python
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

mayuscula_udf = udf(mayuscula, StringType())
```

#### Paso 3: Aplicarla sobre un DataFrame

```python
df.withColumn("nombre_mayus", mayuscula_udf(df["nombre"])).show()
```

### üß† Tipos de datos soportados en UDFs

PySpark permite especificar los tipos de retorno mediante tipos de `pyspark.sql.types` como:

* `StringType()`
* `IntegerType()`
* `FloatType()`
* `BooleanType()`
* `ArrayType()`, `MapType()`, `StructType()` (para estructuras complejas)

### ‚ö° UDFs con `@udf` (decorador)

Otra forma m√°s compacta de definir UDFs:

```python
from pyspark.sql.functions import udf

@udf(returnType=StringType())
def reverse_string(s):
    return s[::-1]
```

### üßä UDFs con m√∫ltiples argumentos

```python
@udf(returnType=StringType())
def concat_nombre_apellido(nombre, apellido):
    return f"{nombre} {apellido}"
```

```python
df.withColumn("nombre_completo", concat_nombre_apellido("nombre", "apellido"))
```

### üß† Pandas UDFs (m√°s eficientes)

**Pandas UDFs** permiten definir funciones personalizadas usando Pandas y son mucho m√°s r√°pidas porque aprovechan Apache Arrow para mejorar el rendimiento.

```python
from pyspark.sql.functions import pandas_udf
import pandas as pd

@pandas_udf(StringType())
def reverse_pandas(col: pd.Series) -> pd.Series:
    return col.str[::-1]
```

```python
df.withColumn("reversa", reverse_pandas("texto"))
```

### ‚ö†Ô∏è Consideraciones importantes

| Aspecto | UDF est√°ndar | Pandas UDF |
|---|---|---|
| Rendimiento | Bajo (no optimizable) | Alto (usa Apache Arrow) |
| Serializaci√≥n | Necesaria (costosa) | Optimizada |
| Compatibilidad SQL | Limitada | Limitada |
| Recomendaci√≥n | Evitar si hay funciones nativas | Preferir sobre UDF est√°ndar |

### ‚úÖ Buenas pr√°cticas

* Usa funciones integradas de Spark siempre que sea posible.
* Si necesitas l√≥gica compleja, considera **Pandas UDF** antes que UDF est√°ndar.
* Documenta claramente el tipo de retorno de cada UDF.
* Evita aplicar UDFs dentro de filtros (`filter()`), ya que se ejecutan fila por fila.
* Eval√∫a el impacto de rendimiento con `explain()` y pruebas controladas.