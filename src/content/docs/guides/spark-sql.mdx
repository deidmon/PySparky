---
title: Spark SQL
description: Uso avanzado de Spark SQL en PySpark.
---


### ğŸ§  Â¿QuÃ© es Spark SQL?

**Spark SQL** es un mÃ³dulo de Apache Spark para el procesamiento de datos estructurados. Permite ejecutar consultas SQL sobre DataFrames y RDDs, integrando lo mejor del mundo de SQL con la escalabilidad de Spark.

### âœ… Ventajas de usar Spark SQL

* Permite usar **sintaxis SQL estÃ¡ndar** sobre datos distribuidos.
* Es compatible con **DataFrames** y tablas temporales.
* Puede optimizar consultas mediante el **Catalyst Optimizer**.
* Soporta integraciÃ³n con mÃºltiples fuentes de datos (Parquet, JSON, Hive, JDBC, etc.).

### ğŸ§ª Crear una sesiÃ³n de Spark con SQL

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Ejemplo Spark SQL") \
    .getOrCreate()
```

### ğŸ“˜ Crear DataFrames y registrarlos como tablas

```python
df = spark.read.json("empleados.json")
df.createOrReplaceTempView("empleados")
```

Esto permite ejecutar consultas SQL sobre `empleados`.

### ğŸ§¾ Consultas SQL

```python
resultado = spark.sql("""
    SELECT nombre, salario 
    FROM empleados 
    WHERE salario > 3000
""")
resultado.show()
```

### ğŸ” Convertir RDD a DataFrame para SQL

```python
from pyspark.sql import Row

rdd = spark.sparkContext.parallelize([
    Row(nombre="Ana", edad=25),
    Row(nombre="Luis", edad=30)
])

df = spark.createDataFrame(rdd)
df.createOrReplaceTempView("personas")

spark.sql("SELECT * FROM personas WHERE edad > 26").show()
```

### ğŸ§© Usar funciones SQL en PySpark

Puedes utilizar muchas funciones SQL comunes directamente en cÃ³digo PySpark, como:

* `F.col("columna")`
* `F.lit(10)`
* `F.upper()`, `F.lower()`
* `F.when()`, `F.otherwise()`
* `F.avg()`, `F.count()`, `F.sum()`

Ejemplo:

```python
from pyspark.sql import functions as F

df.select(
    "nombre",
    F.upper("nombre").alias("nombre_mayus")
).show()
```

### ğŸ§® Consultas con agregaciones

```python
spark.sql("""
    SELECT departamento, AVG(salario) as salario_promedio
    FROM empleados
    GROUP BY departamento
""").show()
```

### ğŸ§¾ Operaciones JOIN con SQL

```python
spark.sql("""
    SELECT e.nombre, d.nombre as departamento
    FROM empleados e
    JOIN departamentos d ON e.dep_id = d.id
""").show()
```

### ğŸ§± Crear tablas permanentes

Para consultas persistentes en mÃºltiples sesiones:

```python
df.write.saveAsTable("tabla_empleados")
```

### ğŸ—ƒï¸ Soporte para fuentes externas

Puedes registrar una tabla basada en archivos externos:

```python
df = spark.read.csv("ventas.csv", header=True, inferSchema=True)
df.createOrReplaceTempView("ventas")

spark.sql("SELECT * FROM ventas WHERE monto > 1000").show()
```

TambiÃ©n puedes conectarte a bases de datos relacionales con JDBC y consultarlas con SQL desde Spark.

### ğŸ“ˆ Plan de ejecuciÃ³n

Para ver cÃ³mo se ejecuta una consulta SQL:

```python
spark.sql("SELECT * FROM empleados").explain()
```

Esto muestra el plan lÃ³gico y fÃ­sico de ejecuciÃ³n generado por **Catalyst Optimizer**.

### ğŸ”’ Consideraciones

* **SQL es declarativo**, ideal para usuarios no familiarizados con programaciÃ³n funcional.
* Las **consultas SQL pueden integrarse con transformaciones DataFrame**.
* Se recomienda usar **Spark SQL o funciones integradas** en lugar de UDFs para mayor rendimiento.