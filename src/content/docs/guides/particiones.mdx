---
title: Manejo de particiones
description: C贸mo PySpark gestiona las particiones de datos.
---

El manejo eficiente de particiones en PySpark es fundamental para lograr un buen rendimiento al trabajar con grandes vol煤menes de datos distribuidos.

### П 驴Qu茅 es una partici贸n?

Una **partici贸n** en PySpark representa una divisi贸n l贸gica de los datos que permite que se procesen en paralelo en un cl煤ster de Spark. Cada partici贸n se procesa por separado en un executor.

### 锔 驴Por qu茅 son importantes?

* Mejoran la paralelizaci贸n.
* Impactan en el uso de memoria y procesamiento.
* Afectan el rendimiento de lecturas y escrituras.

###  Comandos clave para trabajar con particiones

```python
# N煤mero de particiones de un DataFrame
df.rdd.getNumPartitions()

# Reparticionar (shuffle)
df_repart = df.repartition(4)

# Coalescer (sin shuffle)
df_coalesce = df.coalesce(2)
```

###  Diferencia entre repartition() y coalesce()

| M茅todo | Shuffle | Uso recomendado |
|---|---|---|
| `repartition()` | S铆 | Aumentar n煤mero de particiones |
| `coalesce()` | No | Reducir n煤mero de particiones |

###  Particionamiento al escribir archivos

Puedes particionar tus datos al momento de escribirlos para optimizar consultas posteriores:

```python
df.write.partitionBy("anio", "mes").parquet("/ruta/destino")
```

Esto crea una carpeta por cada combinaci贸n de valores de las columnas particionadas, 煤til en consultas donde se filtra por esas columnas.

###  Buenas pr谩cticas

* Usa `repartition()` si vas a hacer joins o agregaciones pesadas.
* Usa `coalesce()` al guardar archivos grandes en menos particiones.
* No uses muchas particiones peque帽as (overhead innecesario).
* Particiona por columnas con alta cardinalidad para evitar skew.
* Evita particionar por columnas con demasiados valores 煤nicos si no vas a filtrarlas despu茅s.

###  Visualizaci贸n de particiones (debug)

```python
df.rdd.glom().map(len).collect()
```

Permite ver cu谩ntos registros hay en cada partici贸n.

### И Ejemplo pr谩ctico

```python
df = spark.range(0, 100).repartition(5)
print(df.rdd.getNumPartitions())  # 5

df2 = df.coalesce(2)
print(df2.rdd.getNumPartitions())  # 2
```