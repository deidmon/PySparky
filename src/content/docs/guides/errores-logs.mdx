---
title: Manejo de errores y logs
description: C√≥mo gestionar errores y logs en PySpark.
---

### üß† ¬øPor qu√© es importante?

Cuando trabajamos con datos distribuidos en PySpark, los errores pueden ser dif√≠ciles de detectar y depurar si no se manejan adecuadamente. Adem√°s, el **registro de logs** es esencial para entender c√≥mo se est√° comportando una aplicaci√≥n y poder hacer un seguimiento eficiente en producci√≥n.

### ‚úÖ Objetivos clave

* Detectar y manejar errores durante el procesamiento distribuido.
* Utilizar los logs para depurar y monitorear procesos.
* Capturar excepciones y registrar fallos o advertencias.

### üõë Manejo de errores (try-except)

Puedes capturar errores de forma tradicional en Python con bloques `try-except`, especialmente √∫til cuando trabajas con funciones personalizadas o transformaciones complejas.

```python
try:
    df = spark.read.csv("archivo_inexistente.csv")
except Exception as e:
    print("Error al leer el archivo:", e)
```

### üß™ Validaci√≥n de datos antes del procesamiento

Es buena pr√°ctica validar tipos y columnas antes de realizar transformaciones:

```python
if "edad" in df.columns:
    df = df.filter(df.edad > 18)
else:
    print("La columna 'edad' no existe.")
```

### üêû UDFs y manejo de errores

Si usas funciones UDF, debes incluir validaci√≥n dentro de ellas:

```python
from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType

def safe_divide(x):
    try:
        return 10 / x
    except Exception:
        return None

divide_udf = udf(safe_divide, IntegerType())
df = df.withColumn("resultado", divide_udf("columna"))
```

### üìã Logs en PySpark

#### Configurar nivel de log

Puedes ajustar el nivel de log para reducir la verbosidad o mostrar solo lo necesario:

```python
spark.sparkContext.setLogLevel("WARN")
```

Niveles comunes:
* `"ALL"`
* `"DEBUG"`
* `"INFO"`
* `"WARN"`
* `"ERROR"`
* `"FATAL"`
* `"OFF"`

### üìÑ Ver los logs de ejecuci√≥n

Cuando corres PySpark localmente o en un cl√∫ster, Spark genera registros con informaci√≥n como:

* Ejecuci√≥n de tareas (stages/tasks)
* Errores de lectura/escritura
* Fallos de nodos
* Optimizaci√≥n de queries

Estos logs suelen verse en consola o en el **Spark UI**.

### üß∞ Uso del Spark UI (modo cl√∫ster o local)

El **Spark Web UI** (por defecto en `http://localhost:4040`) te permite:

* Ver tareas y etapas ejecutadas.
* Identificar cuellos de botella.
* Ver errores espec√≠ficos por partici√≥n.
* Revisar los DAGs y el plan de ejecuci√≥n.

### üßº Buenas pr√°cticas

* ‚úÖ Validar datos antes de transformarlos.
* ‚úÖ Utilizar `try-except` en operaciones cr√≠ticas.
* ‚úÖ Registrar errores con `print()` o bibliotecas de logging.
* ‚úÖ Evitar errores silenciosos en UDFs.
* ‚úÖ Revisar constantemente los logs del Spark UI.
* ‚úÖ Establecer el log level adecuado (`WARN` o `ERROR` en producci√≥n).

### üìò Logging con la biblioteca `logging` de Python

Para tener control m√°s estructurado:

```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    df = spark.read.json("datos.json")
    logger.info("Lectura completada")
except Exception as e:
    logger.error("Fallo al leer el archivo JSON", exc_info=True)
```